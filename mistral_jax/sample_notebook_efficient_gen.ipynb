{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4313c140-421d-4f1f-a283-3461b8db70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "\n",
    "from functools import partial\n",
    "from equinox._misc import default_floating_dtype\n",
    "from jaxtyping import Array, Float, Scalar\n",
    "from typing import Optional, Tuple, List, NamedTuple\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672df14d-d052-403a-8b68-b94a6240abdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CPU for torch\n",
    "device  = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7533de2e-9d14-411a-a55a-852cb62646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model dict, and check if any GPU is used\n",
    "state_dict = torch.load(\"mistral-7B-v0.1/consolidated.00.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27c428-fd21-41a1-9a5c-99a966f5a2a3",
   "metadata": {},
   "source": [
    "# 1. Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd26d27d-8e7e-46e9-ba8d-187a8a57a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        self._model = SentencePieceProcessor(model_file=model_path)\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._model.eos_id()\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self._model.pad_id()\n",
    "\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        return [self._model.bos_id(), *self._model.encode(s)]\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self._model.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5aced-2da9-42df-900d-b9d11d5f45fa",
   "metadata": {},
   "source": [
    "# 2. RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431e5fa1-25dd-401a-abfb-1371f5f1b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_frequencies(dim, max_pos, theta=10000.0):\n",
    "    inv_freq = 1.0 / (\n",
    "        theta ** (jnp.arange(0, dim, 2, dtype=jnp.float32)[: (dim // 2)] / dim)\n",
    "    )\n",
    "    t = jnp.arange(0, max_pos, dtype=jnp.float32)\n",
    "    freqs = jnp.outer(t, inv_freq)\n",
    "    return jnp.cos(freqs), jnp.sin(freqs)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(3,))\n",
    "def calculate_rope(x, cos_freq, sin_freq, offset=0):\n",
    "    # x shape  is [seqlen, num_heads, heads_dim]\n",
    "\n",
    "    # Get the sequence length\n",
    "    seqlen = x.shape[0]\n",
    "\n",
    "    # Get the corresponding positional embeddings\n",
    "    sin = sin_freq[offset : offset + seqlen, :]\n",
    "    cos = cos_freq[offset : offset + seqlen, :]\n",
    "\n",
    "    # Positional embeddings are 2D while our input is 3D\n",
    "    # if `num_heads` dimension is present in the inputs.\n",
    "    # We need to add another dimension to our positional embeddings\n",
    "    sin = sin[:, jnp.newaxis, :]\n",
    "    cos = cos[:, jnp.newaxis, :]\n",
    "\n",
    "    # Get the even-odd positions from the inputs\n",
    "    x1 = x[..., 0::2]\n",
    "    x2 = x[..., 1::2]\n",
    "\n",
    "    # Matmul with the rotation matrix\n",
    "    # [cos_nθ, -sin_nθ] [x1]\n",
    "    # [sin_nθ,  cos_nθ] [x2]\n",
    "    # => [x1 * cos_nθ - x2 * sin_nθ, x1 * sin_nθ + x2 * cos_nθ]\n",
    "    pos_embed = jnp.stack([x1 * cos - x2 * sin, x1 * sin + x2 * cos], axis=-1)\n",
    "    pos_embed = jax.lax.collapse(pos_embed, -2)\n",
    "    return pos_embed.astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1aa31-614e-4483-8599-c5f0b4623292",
   "metadata": {},
   "source": [
    "# 3. RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180fbb2d-ce6f-4b1a-a198-e4f37fab93b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(eqx.Module):\n",
    "    eps: float\n",
    "    weight: Float[Array, \"*shape\"]\n",
    "\n",
    "    def __init__(self, dim, eps, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        self.eps = eps\n",
    "        self.weight = jnp.ones(shape=dim, dtype=dtype)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * jax.lax.rsqrt(jnp.mean(x **2 , keepdims=True) + self.eps)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = self._norm(x.astype(jnp.float32)).astype(x.dtype)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f9a21-bb3c-45e8-805c-5f8605f72423",
   "metadata": {},
   "source": [
    "# 4. FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efffa74f-556b-4c3e-9f73-e23acfa1da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(eqx.Module):\n",
    "    w1: eqx.nn.Linear\n",
    "    w2: eqx.nn.Linear\n",
    "    w3: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.w1 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.w2 = eqx.nn.Linear(args.hidden_dim, args.dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.w3 = eqx.nn.Linear(args.dim, args.hidden_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = jax.nn.silu(self.w1(x).astype(jnp.float32)).astype(x.dtype)\n",
    "        return self.w2(h * self.w3(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c459d5-b30d-48e5-924b-ea661542e8a2",
   "metadata": {},
   "source": [
    "# 5. Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2048b724-3015-43c8-be5e-0214eb83af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    head_dim: int\n",
    "    n_kv_heads: int\n",
    "    kv_repeats: int\n",
    "    sliding_window: int\n",
    "    scale: float\n",
    "    wq: eqx.nn.Linear\n",
    "    wk: eqx.nn.Linear\n",
    "    wv: eqx.nn.Linear\n",
    "    wo: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        dtype = default_floating_dtype if dtype is None else dtype\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.head_dim\n",
    "        self.n_kv_heads = args.n_kv_heads\n",
    "        self.dim = args.dim\n",
    "        self.kv_repeats = self.n_heads // self.n_kv_heads\n",
    "        self.sliding_window = args.sliding_window\n",
    "\n",
    "        self.scale = args.head_dim**-0.5\n",
    "\n",
    "        self.wq = eqx.nn.Linear(args.dim, args.n_heads * args.head_dim, use_bias=False, key=key1, dtype=dtype)\n",
    "        self.wk = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key2, dtype=dtype)\n",
    "        self.wv = eqx.nn.Linear(args.dim, args.n_kv_heads * args.head_dim, use_bias=False, key=key3, dtype=dtype)\n",
    "        self.wo = eqx.nn.Linear(args.n_heads * args.head_dim, args.dim, use_bias=False, key=key4, dtype=dtype)\n",
    "\n",
    "    def compute_scores_and_output(self, xq, key, value, mask, seqlen, pos_mask):\n",
    "        query = jnp.transpose(xq, (1, 0, 2))\n",
    "        key = jnp.transpose(key, (1, 0, 2))\n",
    "        value = jnp.transpose(value, (1, 0, 2))\n",
    "\n",
    "        # # # scores : [n_heads, seqlen | 1, seqlen]\n",
    "        scores = jnp.matmul(query, jnp.transpose(key, (0, 2, 1))) * self.scale\n",
    "        if pos_mask is not None:\n",
    "            scores = jnp.where(pos_mask, -jnp.inf, scores)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Mask will of shape [seqlen, seqlen] but our scores\n",
    "            # have shape [num_heads, seqlen, seqlen], hence we need\n",
    "            # to introduce another dimension in the mask\n",
    "            mask = mask[jnp.newaxis, ...]\n",
    "            scores = scores + mask\n",
    "\n",
    "        scores = jax.nn.softmax(scores.astype(jnp.float32), axis=-1).astype(query.dtype)\n",
    "        output = jnp.matmul(scores, value)\n",
    "        output = jnp.reshape(jnp.transpose(output, (1, 0, 2)), (seqlen, -1))\n",
    "        output = jax.vmap(self.wo)(output)\n",
    "        return output\n",
    "\n",
    "    def __call__(self,  x, cos_freq, sin_freq, positions, mask=None, cache_k=None, cache_v=None):\n",
    "        # x shape: [seqlen, embed_dim]\n",
    "        seqlen = x.shape[0]\n",
    "        \n",
    "        xq = jax.vmap(self.wq)(x)\n",
    "        xk = jax.vmap(self.wk)(x)\n",
    "        xv = jax.vmap(self.wv)(x)\n",
    "\n",
    "        xq = jnp.reshape(xq, (seqlen, self.n_heads, self.head_dim))\n",
    "        xk = jnp.reshape(xk, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "        xv = jnp.reshape(xv, (seqlen, self.n_kv_heads, self.head_dim))\n",
    "\n",
    "        xq = calculate_rope(xq, cos_freq, sin_freq, 0)\n",
    "        xk = calculate_rope(xk, cos_freq, sin_freq, 0)\n",
    "\n",
    "        if positions.shape[0] > 1:\n",
    "            # prefill\n",
    "            cache_k = cache_k.at[positions, :, :].set(xk[positions, :, :], mode=\"drop\")\n",
    "            cache_v = cache_v.at[positions, :, :].set(xv[positions, :, :], mode=\"drop\")\n",
    "            key = jnp.repeat(xk, self.kv_repeats, axis=1)\n",
    "            value = jnp.repeat(xv, self.kv_repeats, axis=1)\n",
    "            output = self.compute_scores_and_output(xq, key, value, mask, seqlen, None)\n",
    "        else:\n",
    "            # single-token generation\n",
    "            one_hot_indices = jax.nn.one_hot(positions, self.sliding_window, dtype=cache_k.dtype).reshape(self.sliding_window, 1, 1)\n",
    "            # the `where` update is only necessary if you are calling the cache multiple times with the same prompt\n",
    "            # Ideally, we expect that you flush out the cache with the new prompt, and start over.\n",
    "            # What does this do? It ensures that we are not adding any values updated earlier \n",
    "            # with the new updates, meaning we are always replacing the value not updating it.\n",
    "            # For example, if prompt had a length of 6, and you want to generate 7th token, this\n",
    "            # ensures that we are not adding the old value of 7th token to the updated value as\n",
    "            # it would lead to wrong results. \n",
    "            # In case, you are flusing the caceh after every prompt, remove the `jnp.where()` condition\n",
    "            # and pass the updates directly to cache_k, and cache_v respectively \n",
    "            # i.e. cache_k = cache_k + xk * one_hot_indices\n",
    "            # and cache_v = cache_v + xv * one_hot_indices\n",
    "            k_updates = cache_k + xk * one_hot_indices\n",
    "            v_updates = cache_v + xv * one_hot_indices\n",
    "            cache_k = jnp.where(cache_k, cache_k, k_updates)\n",
    "            cache_v = jnp.where(cache_v, cache_v, v_updates)\n",
    "        \n",
    "            cur_pos = positions[-1] + 1\n",
    "            causal_mask = jnp.broadcast_to(jnp.arange(self.sliding_window) >= cur_pos,(1, 1, self.sliding_window)).reshape(self.sliding_window,1,1)\n",
    "            key = jnp.repeat(jnp.where(causal_mask, 0, cache_k), axis=1, repeats=self.kv_repeats)\n",
    "            value = jnp.repeat(jnp.where(causal_mask, 0, cache_v), axis=1, repeats=self.kv_repeats)\n",
    "            output = self.compute_scores_and_output(xq, key, value, mask, seqlen, causal_mask.reshape((1, 1, self.sliding_window)))\n",
    "\n",
    "        return output, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a123d-c4b1-44f0-a6cd-5daf67c63adb",
   "metadata": {},
   "source": [
    "# 6. TransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f84cb56f-1a8f-4c28-9f3e-2c180f5e86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(eqx.Module):\n",
    "    dim: int\n",
    "    n_heads: int\n",
    "    attention: Attention\n",
    "    attention_norm: RMSNorm\n",
    "    feed_forward: FeedForward\n",
    "    ffn_norm: RMSNorm\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "\n",
    "        self.attention = Attention(args, key=key1, dtype=dtype)\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "        self.feed_forward = FeedForward(args, key=key2, dtype=dtype)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "        normed_x = jax.vmap(self.attention_norm)(x)\n",
    "        r, cache_k, cache_v = self.attention(normed_x, cos_freq, sin_freq, positions, mask, cache_k, cache_v)\n",
    "        h = x + r\n",
    "        r = jax.vmap(self.feed_forward)(jax.vmap(self.ffn_norm)(h))\n",
    "        out = h +r\n",
    "        return out, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f9502-010b-42ca-86ac-666e9476ff5c",
   "metadata": {},
   "source": [
    "# 7. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdec9a64-57c4-4fb5-8e8a-c3ecb4e5bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    tok_embeddings: eqx.nn.Embedding\n",
    "    layers: TransformerBlock\n",
    "    norm: RMSNorm\n",
    "    output: eqx.nn.Linear\n",
    "    vocab_size: int\n",
    "    n_layers: int\n",
    "    sliding_window: int\n",
    "\n",
    "    def __init__(self, args, key, dtype=jnp.bfloat16):\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "        self.sliding_window = args.sliding_window\n",
    "        keys = jax.random.split(key, args.n_layers + 2)\n",
    "        embed_key, linear_key, tf_layers_keys = keys[0], keys[1], keys[2:]\n",
    "\n",
    "        self.tok_embeddings = eqx.nn.Embedding(args.vocab_size, args.dim, key=embed_key, dtype=dtype)\n",
    "        self.norm = RMSNorm(dim=args.dim, eps=args.norm_eps, dtype=dtype)\n",
    "        self.output = eqx.nn.Linear(args.dim, args.vocab_size, use_bias=False, key=linear_key, dtype=dtype)\n",
    "        self.layers = [TransformerBlock(args, key=tf_layers_keys[i], dtype=dtype) for i in range(args.n_layers)] \n",
    "\n",
    "    def compute_mask(self, seqlen):\n",
    "        t = jnp.full((seqlen, seqlen), dtype=jnp.bfloat16, fill_value=1)\n",
    "        mask = jnp.tril(t, k=0)\n",
    "        # make the mask banded to account for sliding window\n",
    "        mask = jnp.triu(mask, k=-self.sliding_window)\n",
    "        mask = jnp.log(mask)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def __call__(self, x, cos_freq, sin_freq, positions, mask, cache_k, cache_v):\n",
    "        # x is of shape (seqlen, )\n",
    "        h = jax.vmap(self.tok_embeddings)(x)\n",
    "        \n",
    "        if x.shape[-1] > 1:\n",
    "            seqlen = x.shape[-1]\n",
    "            mask = self.compute_mask(seqlen)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h, cache_ki, cache_vi = layer(h, cos_freq, sin_freq, positions, mask, cache_k[i, ...], cache_v[i, ...])\n",
    "            cache_k = cache_k.at[i, :, :, :].set(cache_ki)\n",
    "            cache_v = cache_v.at[i, :, :, :].set(cache_vi)\n",
    "        \n",
    "        h = jax.vmap(self.norm)(h)\n",
    "        h = jax.vmap(self.output)(h).astype(jnp.float32)\n",
    "        return h, cache_k, cache_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a7c63f-4be6-4bad-b6f5-b77c77bcc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs(NamedTuple):\n",
    "    dim: int\n",
    "    n_layers: int\n",
    "    n_heads: int\n",
    "    n_kv_heads: int\n",
    "    head_dim: int\n",
    "    hidden_dim: int\n",
    "    vocab_size: int\n",
    "    sliding_window: int\n",
    "    norm_eps: float\n",
    "    max_batch_size: int = 1\n",
    "\n",
    "\n",
    "with open('./mistral-7B-v0.1/params.json', 'r') as f:\n",
    "    args = ModelArgs(**json.loads(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a2d2cf-40f6-48af-a70f-37847532e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_weights_from_torch(torch_weights, eqx_model):\n",
    "    def load_weights(path, leaf):\n",
    "        path_pieces = []\n",
    "        for path_elem in path:\n",
    "            if isinstance(path_elem, jax.tree_util.GetAttrKey):\n",
    "                 path_pieces.append(path_elem.name)\n",
    "            elif isinstance(path_elem, jax.tree_util.SequenceKey):\n",
    "                 path_pieces.append(str(path_elem.idx))\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported path type {type(path_elem)}\")\n",
    "\n",
    "        path_pieces = \".\".join(path_pieces)\n",
    "        \n",
    "        if \"weight\" in path_pieces:\n",
    "            weight = torch_weights[path_pieces]\n",
    "            weight = jnp.asarray(weight.float().numpy(), dtype=jnp.bfloat16)\n",
    "            assert weight.shape == leaf.shape\n",
    "            assert weight.dtype == leaf.dtype\n",
    "            return weight\n",
    "        else:\n",
    "            print(f\"Weights not ported for: {path_pieces}\")\n",
    "            return leaf\n",
    "\n",
    "    return jax.tree_util.tree_map_with_path(load_weights, eqx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67530c78-98a9-4f84-aa4b-af0819b5f5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights not ported for: layers.0.dim\n",
      "Weights not ported for: layers.0.n_heads\n",
      "Weights not ported for: layers.0.attention.dim\n",
      "Weights not ported for: layers.0.attention.n_heads\n",
      "Weights not ported for: layers.0.attention.head_dim\n",
      "Weights not ported for: layers.0.attention.n_kv_heads\n",
      "Weights not ported for: layers.0.attention.kv_repeats\n",
      "Weights not ported for: layers.0.attention.sliding_window\n",
      "Weights not ported for: layers.0.attention.scale\n",
      "Weights not ported for: layers.0.attention_norm.eps\n",
      "Weights not ported for: layers.0.ffn_norm.eps\n",
      "Weights not ported for: layers.1.dim\n",
      "Weights not ported for: layers.1.n_heads\n",
      "Weights not ported for: layers.1.attention.dim\n",
      "Weights not ported for: layers.1.attention.n_heads\n",
      "Weights not ported for: layers.1.attention.head_dim\n",
      "Weights not ported for: layers.1.attention.n_kv_heads\n",
      "Weights not ported for: layers.1.attention.kv_repeats\n",
      "Weights not ported for: layers.1.attention.sliding_window\n",
      "Weights not ported for: layers.1.attention.scale\n",
      "Weights not ported for: layers.1.attention_norm.eps\n",
      "Weights not ported for: layers.1.ffn_norm.eps\n",
      "Weights not ported for: layers.2.dim\n",
      "Weights not ported for: layers.2.n_heads\n",
      "Weights not ported for: layers.2.attention.dim\n",
      "Weights not ported for: layers.2.attention.n_heads\n",
      "Weights not ported for: layers.2.attention.head_dim\n",
      "Weights not ported for: layers.2.attention.n_kv_heads\n",
      "Weights not ported for: layers.2.attention.kv_repeats\n",
      "Weights not ported for: layers.2.attention.sliding_window\n",
      "Weights not ported for: layers.2.attention.scale\n",
      "Weights not ported for: layers.2.attention_norm.eps\n",
      "Weights not ported for: layers.2.ffn_norm.eps\n",
      "Weights not ported for: layers.3.dim\n",
      "Weights not ported for: layers.3.n_heads\n",
      "Weights not ported for: layers.3.attention.dim\n",
      "Weights not ported for: layers.3.attention.n_heads\n",
      "Weights not ported for: layers.3.attention.head_dim\n",
      "Weights not ported for: layers.3.attention.n_kv_heads\n",
      "Weights not ported for: layers.3.attention.kv_repeats\n",
      "Weights not ported for: layers.3.attention.sliding_window\n",
      "Weights not ported for: layers.3.attention.scale\n",
      "Weights not ported for: layers.3.attention_norm.eps\n",
      "Weights not ported for: layers.3.ffn_norm.eps\n",
      "Weights not ported for: layers.4.dim\n",
      "Weights not ported for: layers.4.n_heads\n",
      "Weights not ported for: layers.4.attention.dim\n",
      "Weights not ported for: layers.4.attention.n_heads\n",
      "Weights not ported for: layers.4.attention.head_dim\n",
      "Weights not ported for: layers.4.attention.n_kv_heads\n",
      "Weights not ported for: layers.4.attention.kv_repeats\n",
      "Weights not ported for: layers.4.attention.sliding_window\n",
      "Weights not ported for: layers.4.attention.scale\n",
      "Weights not ported for: layers.4.attention_norm.eps\n",
      "Weights not ported for: layers.4.ffn_norm.eps\n",
      "Weights not ported for: layers.5.dim\n",
      "Weights not ported for: layers.5.n_heads\n",
      "Weights not ported for: layers.5.attention.dim\n",
      "Weights not ported for: layers.5.attention.n_heads\n",
      "Weights not ported for: layers.5.attention.head_dim\n",
      "Weights not ported for: layers.5.attention.n_kv_heads\n",
      "Weights not ported for: layers.5.attention.kv_repeats\n",
      "Weights not ported for: layers.5.attention.sliding_window\n",
      "Weights not ported for: layers.5.attention.scale\n",
      "Weights not ported for: layers.5.attention_norm.eps\n",
      "Weights not ported for: layers.5.ffn_norm.eps\n",
      "Weights not ported for: layers.6.dim\n",
      "Weights not ported for: layers.6.n_heads\n",
      "Weights not ported for: layers.6.attention.dim\n",
      "Weights not ported for: layers.6.attention.n_heads\n",
      "Weights not ported for: layers.6.attention.head_dim\n",
      "Weights not ported for: layers.6.attention.n_kv_heads\n",
      "Weights not ported for: layers.6.attention.kv_repeats\n",
      "Weights not ported for: layers.6.attention.sliding_window\n",
      "Weights not ported for: layers.6.attention.scale\n",
      "Weights not ported for: layers.6.attention_norm.eps\n",
      "Weights not ported for: layers.6.ffn_norm.eps\n",
      "Weights not ported for: layers.7.dim\n",
      "Weights not ported for: layers.7.n_heads\n",
      "Weights not ported for: layers.7.attention.dim\n",
      "Weights not ported for: layers.7.attention.n_heads\n",
      "Weights not ported for: layers.7.attention.head_dim\n",
      "Weights not ported for: layers.7.attention.n_kv_heads\n",
      "Weights not ported for: layers.7.attention.kv_repeats\n",
      "Weights not ported for: layers.7.attention.sliding_window\n",
      "Weights not ported for: layers.7.attention.scale\n",
      "Weights not ported for: layers.7.attention_norm.eps\n",
      "Weights not ported for: layers.7.ffn_norm.eps\n",
      "Weights not ported for: layers.8.dim\n",
      "Weights not ported for: layers.8.n_heads\n",
      "Weights not ported for: layers.8.attention.dim\n",
      "Weights not ported for: layers.8.attention.n_heads\n",
      "Weights not ported for: layers.8.attention.head_dim\n",
      "Weights not ported for: layers.8.attention.n_kv_heads\n",
      "Weights not ported for: layers.8.attention.kv_repeats\n",
      "Weights not ported for: layers.8.attention.sliding_window\n",
      "Weights not ported for: layers.8.attention.scale\n",
      "Weights not ported for: layers.8.attention_norm.eps\n",
      "Weights not ported for: layers.8.ffn_norm.eps\n",
      "Weights not ported for: layers.9.dim\n",
      "Weights not ported for: layers.9.n_heads\n",
      "Weights not ported for: layers.9.attention.dim\n",
      "Weights not ported for: layers.9.attention.n_heads\n",
      "Weights not ported for: layers.9.attention.head_dim\n",
      "Weights not ported for: layers.9.attention.n_kv_heads\n",
      "Weights not ported for: layers.9.attention.kv_repeats\n",
      "Weights not ported for: layers.9.attention.sliding_window\n",
      "Weights not ported for: layers.9.attention.scale\n",
      "Weights not ported for: layers.9.attention_norm.eps\n",
      "Weights not ported for: layers.9.ffn_norm.eps\n",
      "Weights not ported for: layers.10.dim\n",
      "Weights not ported for: layers.10.n_heads\n",
      "Weights not ported for: layers.10.attention.dim\n",
      "Weights not ported for: layers.10.attention.n_heads\n",
      "Weights not ported for: layers.10.attention.head_dim\n",
      "Weights not ported for: layers.10.attention.n_kv_heads\n",
      "Weights not ported for: layers.10.attention.kv_repeats\n",
      "Weights not ported for: layers.10.attention.sliding_window\n",
      "Weights not ported for: layers.10.attention.scale\n",
      "Weights not ported for: layers.10.attention_norm.eps\n",
      "Weights not ported for: layers.10.ffn_norm.eps\n",
      "Weights not ported for: layers.11.dim\n",
      "Weights not ported for: layers.11.n_heads\n",
      "Weights not ported for: layers.11.attention.dim\n",
      "Weights not ported for: layers.11.attention.n_heads\n",
      "Weights not ported for: layers.11.attention.head_dim\n",
      "Weights not ported for: layers.11.attention.n_kv_heads\n",
      "Weights not ported for: layers.11.attention.kv_repeats\n",
      "Weights not ported for: layers.11.attention.sliding_window\n",
      "Weights not ported for: layers.11.attention.scale\n",
      "Weights not ported for: layers.11.attention_norm.eps\n",
      "Weights not ported for: layers.11.ffn_norm.eps\n",
      "Weights not ported for: layers.12.dim\n",
      "Weights not ported for: layers.12.n_heads\n",
      "Weights not ported for: layers.12.attention.dim\n",
      "Weights not ported for: layers.12.attention.n_heads\n",
      "Weights not ported for: layers.12.attention.head_dim\n",
      "Weights not ported for: layers.12.attention.n_kv_heads\n",
      "Weights not ported for: layers.12.attention.kv_repeats\n",
      "Weights not ported for: layers.12.attention.sliding_window\n",
      "Weights not ported for: layers.12.attention.scale\n",
      "Weights not ported for: layers.12.attention_norm.eps\n",
      "Weights not ported for: layers.12.ffn_norm.eps\n",
      "Weights not ported for: layers.13.dim\n",
      "Weights not ported for: layers.13.n_heads\n",
      "Weights not ported for: layers.13.attention.dim\n",
      "Weights not ported for: layers.13.attention.n_heads\n",
      "Weights not ported for: layers.13.attention.head_dim\n",
      "Weights not ported for: layers.13.attention.n_kv_heads\n",
      "Weights not ported for: layers.13.attention.kv_repeats\n",
      "Weights not ported for: layers.13.attention.sliding_window\n",
      "Weights not ported for: layers.13.attention.scale\n",
      "Weights not ported for: layers.13.attention_norm.eps\n",
      "Weights not ported for: layers.13.ffn_norm.eps\n",
      "Weights not ported for: layers.14.dim\n",
      "Weights not ported for: layers.14.n_heads\n",
      "Weights not ported for: layers.14.attention.dim\n",
      "Weights not ported for: layers.14.attention.n_heads\n",
      "Weights not ported for: layers.14.attention.head_dim\n",
      "Weights not ported for: layers.14.attention.n_kv_heads\n",
      "Weights not ported for: layers.14.attention.kv_repeats\n",
      "Weights not ported for: layers.14.attention.sliding_window\n",
      "Weights not ported for: layers.14.attention.scale\n",
      "Weights not ported for: layers.14.attention_norm.eps\n",
      "Weights not ported for: layers.14.ffn_norm.eps\n",
      "Weights not ported for: layers.15.dim\n",
      "Weights not ported for: layers.15.n_heads\n",
      "Weights not ported for: layers.15.attention.dim\n",
      "Weights not ported for: layers.15.attention.n_heads\n",
      "Weights not ported for: layers.15.attention.head_dim\n",
      "Weights not ported for: layers.15.attention.n_kv_heads\n",
      "Weights not ported for: layers.15.attention.kv_repeats\n",
      "Weights not ported for: layers.15.attention.sliding_window\n",
      "Weights not ported for: layers.15.attention.scale\n",
      "Weights not ported for: layers.15.attention_norm.eps\n",
      "Weights not ported for: layers.15.ffn_norm.eps\n",
      "Weights not ported for: layers.16.dim\n",
      "Weights not ported for: layers.16.n_heads\n",
      "Weights not ported for: layers.16.attention.dim\n",
      "Weights not ported for: layers.16.attention.n_heads\n",
      "Weights not ported for: layers.16.attention.head_dim\n",
      "Weights not ported for: layers.16.attention.n_kv_heads\n",
      "Weights not ported for: layers.16.attention.kv_repeats\n",
      "Weights not ported for: layers.16.attention.sliding_window\n",
      "Weights not ported for: layers.16.attention.scale\n",
      "Weights not ported for: layers.16.attention_norm.eps\n",
      "Weights not ported for: layers.16.ffn_norm.eps\n",
      "Weights not ported for: layers.17.dim\n",
      "Weights not ported for: layers.17.n_heads\n",
      "Weights not ported for: layers.17.attention.dim\n",
      "Weights not ported for: layers.17.attention.n_heads\n",
      "Weights not ported for: layers.17.attention.head_dim\n",
      "Weights not ported for: layers.17.attention.n_kv_heads\n",
      "Weights not ported for: layers.17.attention.kv_repeats\n",
      "Weights not ported for: layers.17.attention.sliding_window\n",
      "Weights not ported for: layers.17.attention.scale\n",
      "Weights not ported for: layers.17.attention_norm.eps\n",
      "Weights not ported for: layers.17.ffn_norm.eps\n",
      "Weights not ported for: layers.18.dim\n",
      "Weights not ported for: layers.18.n_heads\n",
      "Weights not ported for: layers.18.attention.dim\n",
      "Weights not ported for: layers.18.attention.n_heads\n",
      "Weights not ported for: layers.18.attention.head_dim\n",
      "Weights not ported for: layers.18.attention.n_kv_heads\n",
      "Weights not ported for: layers.18.attention.kv_repeats\n",
      "Weights not ported for: layers.18.attention.sliding_window\n",
      "Weights not ported for: layers.18.attention.scale\n",
      "Weights not ported for: layers.18.attention_norm.eps\n",
      "Weights not ported for: layers.18.ffn_norm.eps\n",
      "Weights not ported for: layers.19.dim\n",
      "Weights not ported for: layers.19.n_heads\n",
      "Weights not ported for: layers.19.attention.dim\n",
      "Weights not ported for: layers.19.attention.n_heads\n",
      "Weights not ported for: layers.19.attention.head_dim\n",
      "Weights not ported for: layers.19.attention.n_kv_heads\n",
      "Weights not ported for: layers.19.attention.kv_repeats\n",
      "Weights not ported for: layers.19.attention.sliding_window\n",
      "Weights not ported for: layers.19.attention.scale\n",
      "Weights not ported for: layers.19.attention_norm.eps\n",
      "Weights not ported for: layers.19.ffn_norm.eps\n",
      "Weights not ported for: layers.20.dim\n",
      "Weights not ported for: layers.20.n_heads\n",
      "Weights not ported for: layers.20.attention.dim\n",
      "Weights not ported for: layers.20.attention.n_heads\n",
      "Weights not ported for: layers.20.attention.head_dim\n",
      "Weights not ported for: layers.20.attention.n_kv_heads\n",
      "Weights not ported for: layers.20.attention.kv_repeats\n",
      "Weights not ported for: layers.20.attention.sliding_window\n",
      "Weights not ported for: layers.20.attention.scale\n",
      "Weights not ported for: layers.20.attention_norm.eps\n",
      "Weights not ported for: layers.20.ffn_norm.eps\n",
      "Weights not ported for: layers.21.dim\n",
      "Weights not ported for: layers.21.n_heads\n",
      "Weights not ported for: layers.21.attention.dim\n",
      "Weights not ported for: layers.21.attention.n_heads\n",
      "Weights not ported for: layers.21.attention.head_dim\n",
      "Weights not ported for: layers.21.attention.n_kv_heads\n",
      "Weights not ported for: layers.21.attention.kv_repeats\n",
      "Weights not ported for: layers.21.attention.sliding_window\n",
      "Weights not ported for: layers.21.attention.scale\n",
      "Weights not ported for: layers.21.attention_norm.eps\n",
      "Weights not ported for: layers.21.ffn_norm.eps\n",
      "Weights not ported for: layers.22.dim\n",
      "Weights not ported for: layers.22.n_heads\n",
      "Weights not ported for: layers.22.attention.dim\n",
      "Weights not ported for: layers.22.attention.n_heads\n",
      "Weights not ported for: layers.22.attention.head_dim\n",
      "Weights not ported for: layers.22.attention.n_kv_heads\n",
      "Weights not ported for: layers.22.attention.kv_repeats\n",
      "Weights not ported for: layers.22.attention.sliding_window\n",
      "Weights not ported for: layers.22.attention.scale\n",
      "Weights not ported for: layers.22.attention_norm.eps\n",
      "Weights not ported for: layers.22.ffn_norm.eps\n",
      "Weights not ported for: layers.23.dim\n",
      "Weights not ported for: layers.23.n_heads\n",
      "Weights not ported for: layers.23.attention.dim\n",
      "Weights not ported for: layers.23.attention.n_heads\n",
      "Weights not ported for: layers.23.attention.head_dim\n",
      "Weights not ported for: layers.23.attention.n_kv_heads\n",
      "Weights not ported for: layers.23.attention.kv_repeats\n",
      "Weights not ported for: layers.23.attention.sliding_window\n",
      "Weights not ported for: layers.23.attention.scale\n",
      "Weights not ported for: layers.23.attention_norm.eps\n",
      "Weights not ported for: layers.23.ffn_norm.eps\n",
      "Weights not ported for: layers.24.dim\n",
      "Weights not ported for: layers.24.n_heads\n",
      "Weights not ported for: layers.24.attention.dim\n",
      "Weights not ported for: layers.24.attention.n_heads\n",
      "Weights not ported for: layers.24.attention.head_dim\n",
      "Weights not ported for: layers.24.attention.n_kv_heads\n",
      "Weights not ported for: layers.24.attention.kv_repeats\n",
      "Weights not ported for: layers.24.attention.sliding_window\n",
      "Weights not ported for: layers.24.attention.scale\n",
      "Weights not ported for: layers.24.attention_norm.eps\n",
      "Weights not ported for: layers.24.ffn_norm.eps\n",
      "Weights not ported for: layers.25.dim\n",
      "Weights not ported for: layers.25.n_heads\n",
      "Weights not ported for: layers.25.attention.dim\n",
      "Weights not ported for: layers.25.attention.n_heads\n",
      "Weights not ported for: layers.25.attention.head_dim\n",
      "Weights not ported for: layers.25.attention.n_kv_heads\n",
      "Weights not ported for: layers.25.attention.kv_repeats\n",
      "Weights not ported for: layers.25.attention.sliding_window\n",
      "Weights not ported for: layers.25.attention.scale\n",
      "Weights not ported for: layers.25.attention_norm.eps\n",
      "Weights not ported for: layers.25.ffn_norm.eps\n",
      "Weights not ported for: layers.26.dim\n",
      "Weights not ported for: layers.26.n_heads\n",
      "Weights not ported for: layers.26.attention.dim\n",
      "Weights not ported for: layers.26.attention.n_heads\n",
      "Weights not ported for: layers.26.attention.head_dim\n",
      "Weights not ported for: layers.26.attention.n_kv_heads\n",
      "Weights not ported for: layers.26.attention.kv_repeats\n",
      "Weights not ported for: layers.26.attention.sliding_window\n",
      "Weights not ported for: layers.26.attention.scale\n",
      "Weights not ported for: layers.26.attention_norm.eps\n",
      "Weights not ported for: layers.26.ffn_norm.eps\n",
      "Weights not ported for: layers.27.dim\n",
      "Weights not ported for: layers.27.n_heads\n",
      "Weights not ported for: layers.27.attention.dim\n",
      "Weights not ported for: layers.27.attention.n_heads\n",
      "Weights not ported for: layers.27.attention.head_dim\n",
      "Weights not ported for: layers.27.attention.n_kv_heads\n",
      "Weights not ported for: layers.27.attention.kv_repeats\n",
      "Weights not ported for: layers.27.attention.sliding_window\n",
      "Weights not ported for: layers.27.attention.scale\n",
      "Weights not ported for: layers.27.attention_norm.eps\n",
      "Weights not ported for: layers.27.ffn_norm.eps\n",
      "Weights not ported for: layers.28.dim\n",
      "Weights not ported for: layers.28.n_heads\n",
      "Weights not ported for: layers.28.attention.dim\n",
      "Weights not ported for: layers.28.attention.n_heads\n",
      "Weights not ported for: layers.28.attention.head_dim\n",
      "Weights not ported for: layers.28.attention.n_kv_heads\n",
      "Weights not ported for: layers.28.attention.kv_repeats\n",
      "Weights not ported for: layers.28.attention.sliding_window\n",
      "Weights not ported for: layers.28.attention.scale\n",
      "Weights not ported for: layers.28.attention_norm.eps\n",
      "Weights not ported for: layers.28.ffn_norm.eps\n",
      "Weights not ported for: layers.29.dim\n",
      "Weights not ported for: layers.29.n_heads\n",
      "Weights not ported for: layers.29.attention.dim\n",
      "Weights not ported for: layers.29.attention.n_heads\n",
      "Weights not ported for: layers.29.attention.head_dim\n",
      "Weights not ported for: layers.29.attention.n_kv_heads\n",
      "Weights not ported for: layers.29.attention.kv_repeats\n",
      "Weights not ported for: layers.29.attention.sliding_window\n",
      "Weights not ported for: layers.29.attention.scale\n",
      "Weights not ported for: layers.29.attention_norm.eps\n",
      "Weights not ported for: layers.29.ffn_norm.eps\n",
      "Weights not ported for: layers.30.dim\n",
      "Weights not ported for: layers.30.n_heads\n",
      "Weights not ported for: layers.30.attention.dim\n",
      "Weights not ported for: layers.30.attention.n_heads\n",
      "Weights not ported for: layers.30.attention.head_dim\n",
      "Weights not ported for: layers.30.attention.n_kv_heads\n",
      "Weights not ported for: layers.30.attention.kv_repeats\n",
      "Weights not ported for: layers.30.attention.sliding_window\n",
      "Weights not ported for: layers.30.attention.scale\n",
      "Weights not ported for: layers.30.attention_norm.eps\n",
      "Weights not ported for: layers.30.ffn_norm.eps\n",
      "Weights not ported for: layers.31.dim\n",
      "Weights not ported for: layers.31.n_heads\n",
      "Weights not ported for: layers.31.attention.dim\n",
      "Weights not ported for: layers.31.attention.n_heads\n",
      "Weights not ported for: layers.31.attention.head_dim\n",
      "Weights not ported for: layers.31.attention.n_kv_heads\n",
      "Weights not ported for: layers.31.attention.kv_repeats\n",
      "Weights not ported for: layers.31.attention.sliding_window\n",
      "Weights not ported for: layers.31.attention.scale\n",
      "Weights not ported for: layers.31.attention_norm.eps\n",
      "Weights not ported for: layers.31.ffn_norm.eps\n",
      "Weights not ported for: norm.eps\n",
      "Weights not ported for: vocab_size\n",
      "Weights not ported for: n_layers\n",
      "Weights not ported for: sliding_window\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(args, key=jax.random.PRNGKey(1), dtype=jnp.bfloat16)\n",
    "model = port_weights_from_torch(state_dict, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7da969f6-eb91-4df5-9976-1db480914a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_k = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "cache_v = jnp.zeros((args.max_batch_size, args.n_layers, args.sliding_window, args.n_kv_heads, args.head_dim), dtype=jnp.bfloat16)\n",
    "cos_freq, sin_freq = precompute_frequencies(args.head_dim, 128000)\n",
    "vmapped = eqx.filter_vmap(eqx.filter_jit(model), in_axes=(0, None, None, None, None, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001d96e5-f58d-4ea4-b878-92abcfcb85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_pos = jnp.array([0, 1, 2, 3, 4], dtype=jnp.int32)\n",
    "fake_inp = jnp.asarray([[1,  832,  349,  265, 1369]], dtype=jnp.int32)\n",
    "fake_mask = None\n",
    "fake_pos_padded = jnp.pad(fake_pos, args.sliding_window - len(fake_pos))\n",
    "\n",
    "# warmup\n",
    "_ = vmapped(fake_inp, cos_freq[fake_pos], sin_freq[fake_pos], fake_pos_padded, fake_mask, cache_k, cache_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c876b9e3-31d9-4b21-9992-8139dfa17cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\"mistral-7B-v0.1/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d150f865-bbc0-4314-b89c-015739d73daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, cache_k, cache_v, max_tokens=36):\n",
    "    # 1. Encode the prompts\n",
    "    prompts = [\"This is a test\"]\n",
    "    encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]\n",
    "    prompt_lens = [len(x) for x in encoded_prompts]\n",
    "    min_prompt_len = min(prompt_lens)\n",
    "    max_prompt_len = max(prompt_lens)\n",
    "\n",
    "    # 2. Using numpy to generate the desired input. Will replace it with something\n",
    "    # better later on\n",
    "    input_tokens = np.full((len(prompts), max_prompt_len), tokenizer.pad_id, dtype=np.int32)\n",
    "    for i, encoded in enumerate(encoded_prompts):\n",
    "        input_tokens[i, :len(encoded)] = jnp.array((encoded))\n",
    "    input_mask = input_tokens != tokenizer.pad_id\n",
    "    cur_pos = min_prompt_len\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. pre-fill\n",
    "    positions = jnp.arange(0, min_prompt_len)\n",
    "    positions_padded = jnp.pad(positions, args.sliding_window - len(positions))\n",
    "    print(\"Prefilling...\")\n",
    "    start = time.time()\n",
    "    logits, cache_k, cache_v = model(\n",
    "        jnp.asarray(input_tokens[:, :min_prompt_len]),\n",
    "        cos_freq[positions],\n",
    "        sin_freq[positions],\n",
    "        positions_padded,\n",
    "        None,\n",
    "        cache_k,\n",
    "        cache_v\n",
    "    )\n",
    "    print(f\"Time taken to prefill: {time.time()- start :.2f} seconds\")\n",
    "    logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    next_token = jnp.argmax(logprobs[:, -1,:], axis=-1)\n",
    "\n",
    "    # 4. Generation\n",
    "    generated = [next_token[0].item()]\n",
    "    print(\"\\nGenerating...\")\n",
    "    overall_start = time.time()\n",
    "    for t in range(max_tokens):\n",
    "        cur_pos+=1\n",
    "        pos = jnp.array([cur_pos])\n",
    "        logits, cache_k, cache_v = logits, cache_k, cache_v = vmapped(\n",
    "            jnp.asarray(next_token[:, None]),\n",
    "            cos_freq[pos],\n",
    "            sin_freq[pos],\n",
    "            pos,\n",
    "            None,\n",
    "            cache_k,\n",
    "            cache_v\n",
    "        )\n",
    "        logprobs = jax.nn.log_softmax(logits, axis=-1)\n",
    "        next_token = jnp.argmax(logprobs[:, -1,:], axis=-1)\n",
    "        generated.append(next_token[0].item())\n",
    "\n",
    "    end = time.time()\n",
    "    res = prompts[0] + \" \" + \"\".join(tokenizer.decode(generated))\n",
    "    print(f\"Time taken to generate {max_tokens} tokens: {end- overall_start:.2f} seconds\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a96c482-3c52-406d-8ec7-3b93d5f34176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefilling...\n",
      "Time taken to prefill: 0.03 seconds\n",
      "\n",
      "Generating...\n",
      "Time taken to generate 30 tokens: 1.21 seconds\n"
     ]
    }
   ],
   "source": [
    "res = generate(vmapped, tokenizer, cache_k, cache_v, max_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cd01942-e3f9-45a3-a37e-b24c3cb5099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test of the emergency broadcast system.\\n\\nThis is only a test.\\n\\nIf this had been an actual emergency, you would have been instructed where'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3bcaa-b148-4a64-9908-d50ab91d7f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
